{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mysql.connector\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "permite cargar los datos desde un excel, seleccionando las hojas deseadas dentro del archivo .xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_datos_desde_excel(archivo, hojas, engine='openpyxl'):\n",
    "    \"\"\"\n",
    "    Carga datos desde un archivo Excel y devuelve un diccionario de DataFrames.\n",
    "\n",
    "    Parameters:\n",
    "    - archivo (str): Ruta del archivo Excel.\n",
    "    - hojas (list): Lista de nombres de hojas a cargar.\n",
    "    - engine (str, optional): Motor de Excel a utilizar. Por defecto, 'openpyxl'.\n",
    "\n",
    "    Returns:\n",
    "    dfs: Un diccionario donde las claves son los nombres de las hojas y los valores son DataFrames correspondientes.\n",
    "\n",
    "    Example:\n",
    "    >>> datos = cargar_datos_desde_excel('archivo.xlsx', ['Hoja1', 'Hoja2'])\n",
    "    >>> df_hoja1 = datos['Hoja1']\n",
    "    >>> df_hoja2 = datos['Hoja2']\n",
    "    \"\"\"\n",
    "    \n",
    "    xls_file = pd.ExcelFile(archivo, engine=engine)\n",
    "    dfs = {}\n",
    "\n",
    "    for hoja in hojas:\n",
    "        df = pd.read_excel(xls_file, hoja) \n",
    "        dfs[hoja] = df\n",
    "\n",
    "    return dfs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analizar_SD(dataframe):\n",
    "    \"\"\"\n",
    "    Analiza la presencia de valores 'SD' en cada columna del DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    dataframe (pd.DataFrame): El DataFrame a analizar.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Un DataFrame que muestra la cantidad y porcentaje de valores 'SD' en cada columna.\n",
    "    \"\"\"\n",
    "    columnas_con_sd = dataframe.columns\n",
    "    resultados = []\n",
    "\n",
    "    for columna in columnas_con_sd:\n",
    "        cantidad_sd = dataframe[columna].eq('SD').sum()\n",
    "        porcentaje_sd = round((cantidad_sd / len(dataframe)) * 100,2)\n",
    "        resultados.append({'Columna': columna, 'Cantidad de SD': cantidad_sd, '% SD x Columna': porcentaje_sd})\n",
    "\n",
    "    resultados_df = pd.DataFrame(resultados)\n",
    "    resultados_con_sd = resultados_df[resultados_df['Cantidad de SD'] > 0]\n",
    "\n",
    "    return resultados_con_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analizar_nan(dataframe):\n",
    "    \"\"\"\n",
    "    Analiza la presencia de valores NaN en cada columna del DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    dataframe (pd.DataFrame): El DataFrame a analizar.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Un DataFrame que muestra la cantidad y porcentaje de valores NaN en cada columna.\n",
    "    \"\"\"\n",
    "    columnas_con_nan = dataframe.columns\n",
    "    resultados = []\n",
    "\n",
    "    for columna in columnas_con_nan:\n",
    "        cantidad_nan = dataframe[columna].isna().sum()\n",
    "        porcentaje_nan = round((cantidad_nan / len(dataframe)) * 100, 2)\n",
    "        resultados.append({'Columna': columna, 'Cantidad de NaN': cantidad_nan, '% NaN x Columna': porcentaje_nan})\n",
    "\n",
    "    resultados_df = pd.DataFrame(resultados)\n",
    "    resultados_con_nan = resultados_df[resultados_df['Cantidad de NaN'] > 0]\n",
    "\n",
    "    return resultados_con_nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(df, \n",
    "                  drop_duplicates=False, \n",
    "                  drop_na=False, \n",
    "                  fill_na=None, \n",
    "                  convert_to_datetime=None, \n",
    "\n",
    "                  uppercase_columns=None, \n",
    "                  lowercase_columns=None, \n",
    "                  titlecase_columns=None, \n",
    "                  strip_spaces=True, \n",
    "\n",
    "                  rename_columns=None, \n",
    "                  drop_columns=None, \n",
    "                  categorize_columns=None, \n",
    "\n",
    "                  replace_values=None, \n",
    "                  new_columns=None, \n",
    "                  new_columns2=None, \n",
    "\n",
    "                  convert_date_columns=None, \n",
    "                  convert_to_int_columns=None, \n",
    "                  convert_to_float=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    - drop_duplicates (bool): Elimina duplicados si es True.\n",
    "      Ejemplo: cleaned_df = data_cleaning(df_tu_data_frame, drop_duplicates=True)\n",
    "      \n",
    "    - drop_na (bool): Elimina filas con valores nulos si es True.\n",
    "      Ejemplo: cleaned_df = data_cleaning(df_tu_data_frame, drop_na=True)\n",
    "      \n",
    "    - fill_na (dict): Un diccionario donde las claves son los nombres de columnas y los valores son valores para rellenar los nulos.\n",
    "      Ejemplo: fill_na_dict = {'gravedad': 'leve'}\n",
    "               cleaned_df = data_cleaning(df_tu_data_frame, fill_na=fill_na_dict)\n",
    "      \n",
    "    - convert_to_datetime (list): Lista de columnas para convertir a tipo de dato datetime.\n",
    "      Ejemplo: columns_to_convert = ['fecha', 'hora']\n",
    "               cleaned_df = data_cleaning(df_tu_data_frame, convert_to_datetime=columns_to_convert)\n",
    "      \n",
    "    - uppercase_columns (list): Lista de columnas para convertir a mayúsculas.\n",
    "      Ejemplo: columns_to_uppercase = ['nombre', 'apellido']\n",
    "               cleaned_df = data_cleaning(df_tu_data_frame, uppercase_columns=columns_to_uppercase)\n",
    "      \n",
    "    - lowercase_columns (list): Lista de columnas para convertir a minúsculas.\n",
    "      Ejemplo: columns_to_lowercase = ['Ciudad', 'Pais']\n",
    "               cleaned_df = data_cleaning(df_tu_data_frame, lowercase_columns=columns_to_lowercase)\n",
    "      \n",
    "    - titlecase_columns (list): Lista de columnas para convertir a formato de título (primera letra en mayúscula, resto en minúscula).\n",
    "      Ejemplo: columns_to_titlecase = ['titulo', 'categoria']\n",
    "               cleaned_df = data_cleaning(df_tu_data_frame, titlecase_columns=columns_to_titlecase)\n",
    "      \n",
    "    - strip_spaces (bool): Elimina espacios en blanco alrededor de los valores de las celdas si es True.\n",
    "      Ejemplo: cleaned_df = data_cleaning(df_tu_data_frame, strip_spaces=True)\n",
    "      \n",
    "    - rename_columns (dict): Un diccionario donde las claves son los nombres de las columnas actuales y los valores son los nuevos nombres.\n",
    "      Ejemplo: rename_dict = {'Vieja_Columna': 'Nueva_Columna'}\n",
    "               cleaned_df = data_cleaning(df_tu_data_frame, rename_columns=rename_dict)\n",
    "      \n",
    "    - drop_columns (list): Lista de columnas para eliminar.\n",
    "      Ejemplo: columns_to_drop = ['columna1', 'columna2']\n",
    "               cleaned_df = data_cleaning(df_tu_data_frame, drop_columns=columns_to_drop)\n",
    "      \n",
    "    - categorize_columns (list): Lista de columnas para convertir a tipo de dato categoría.\n",
    "      Ejemplo: columns_to_categorize = ['categoria1', 'categoria2']\n",
    "               cleaned_df = data_cleaning(df_tu_data_frame, categorize_columns=columns_to_categorize)\n",
    "      \n",
    "    - replace_values (dict): Un diccionario donde las claves son los nombres de las columnas y los valores son diccionarios de reemplazo.\n",
    "      Ejemplo: replace_dict = {'columna1': {'Antiguo1': 'Nuevo1', 'Antiguo2': 'Nuevo2'}}\n",
    "               cleaned_df = data_cleaning(df_tu_data_frame, replace_values=replace_dict)\n",
    "      \n",
    "    - new_columns (dict): Un diccionario donde las claves son los nombres de las nuevas columnas y los valores son valores para esas columnas.\n",
    "      Ejemplo: new_columns_dict = {'nueva_columna': 0}\n",
    "               cleaned_df = data_cleaning(df_tu_data_frame, new_columns=new_columns_dict)\n",
    "      \n",
    "    - new_columns2 (dict): Un diccionario donde las claves son los nombres de las nuevas columnas y los valores son expresiones\n",
    "                            para calcular el contenido de las nuevas columnas basadas en otras columnas existentes. \n",
    "      Ejemplo: {'nueva_columna1': 'columna_existente * 2'}\n",
    "               cleaned_df = data_cleaning(df_tu_data_frame, new_columns2=new_columns_dict)\n",
    "      \n",
    "    - convert_date_columns (dict): Un diccionario donde las claves son los nombres de las columnas y los valores son los formatos de fecha.\n",
    "      Ejemplo: date_columns_dict = {'fecha': '%Y-%m-%d', 'hora': '%H:%M:%S'}\n",
    "               cleaned_df = data_cleaning(df_tu_data_frame, convert_date_columns=date_columns_dict)\n",
    "      \n",
    "    - convert_to_int_columns (list): Lista de columnas para convertir a tipo de dato entero.\n",
    "      Ejemplo: columns_to_int = ['columna1', 'columna2']\n",
    "               cleaned_df = data_cleaning(df_tu_data_frame, convert_to_int_columns=columns_to_int)\n",
    "    \n",
    "    - convert_to_float (list): Lista de columnas para convertir a tipo de dato float.\n",
    "      Ejemplo: columns_to_float = ['columna1', 'columna2']\n",
    "               cleaned_df = data_cleaning(df_tu_data_frame, convert_to_float=columns_to_float)  \n",
    "            \n",
    "    Retorna:\n",
    "    pd.DataFrame: El DataFrame limpio.\n",
    "    \"\"\"\n",
    "\n",
    "    cleaned_df = df.copy()\n",
    "\n",
    "    if drop_duplicates:\n",
    "        cleaned_df.drop_duplicates(inplace=True)\n",
    "        \n",
    "    if drop_na:\n",
    "        cleaned_df.dropna(inplace=True)\n",
    "        \n",
    "    if fill_na:\n",
    "        cleaned_df.fillna(fill_na, inplace=True)\n",
    "\n",
    "    if convert_to_datetime:\n",
    "        for column in convert_to_datetime:\n",
    "            cleaned_df[column] = pd.to_datetime(cleaned_df[column], errors='coerce')\n",
    "\n",
    "    if uppercase_columns:\n",
    "        for column in uppercase_columns:\n",
    "            cleaned_df[column] = cleaned_df[column].str.upper()\n",
    "\n",
    "    if lowercase_columns:\n",
    "        for column in lowercase_columns:\n",
    "            cleaned_df[column] = cleaned_df[column].str.lower()\n",
    "\n",
    "    if titlecase_columns:\n",
    "        for column in titlecase_columns:\n",
    "            cleaned_df[column] = cleaned_df[column].str.title()\n",
    "\n",
    "    if strip_spaces:\n",
    "        cleaned_df = cleaned_df.apply(lambda x: x.str.strip() if isinstance(x, str) else x)\n",
    "\n",
    "    if rename_columns:\n",
    "        cleaned_df.rename(columns=rename_columns, inplace=True)\n",
    "\n",
    "    if drop_columns:\n",
    "        cleaned_df.drop(columns=drop_columns, inplace=True)\n",
    "\n",
    "    if categorize_columns:\n",
    "      for column, replacement_dict in replacements.items():\n",
    "          if column in cleaned_df.columns:\n",
    "              cleaned_df[column] = cleaned_df[column].astype('category').cat.rename_categories(replacement_dict)\n",
    "          else:\n",
    "              print(f\"La columna '{column}' no existe en el DataFrame.\")\n",
    "\n",
    "\n",
    "    if replace_values:\n",
    "        for column, replacements in replace_values.items():\n",
    "            cleaned_df[column] = cleaned_df[column].replace(replacements)\n",
    "\n",
    "    if new_columns:\n",
    "        for column, value in new_columns.items():\n",
    "            cleaned_df[column] = value\n",
    "\n",
    "    if new_columns2:\n",
    "        for new_column, column_expr in new_columns2.items():\n",
    "            if column_expr:\n",
    "                cleaned_df[new_column] = cleaned_df.eval(column_expr)\n",
    "            else:\n",
    "                cleaned_df[new_column] = None\n",
    "\n",
    "    if convert_date_columns:\n",
    "        for column, date_format in convert_date_columns.items():\n",
    "            cleaned_df[column] = pd.to_datetime(cleaned_df[column], format=date_format, errors='coerce')\n",
    "\n",
    "    if convert_to_int_columns:\n",
    "        for column in convert_to_int_columns:\n",
    "            cleaned_df[column] = pd.to_numeric(cleaned_df[column], errors='coerce').astype('Int64')\n",
    "    \n",
    "    if convert_to_float:\n",
    "        for column in convert_to_float:\n",
    "            cleaned_df[column] = cleaned_df[column].astype(float)\n",
    "        \n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analizar_palabra_clave(dataframe, palabra_clave):\n",
    "    \"\"\"\n",
    "    Analiza la presencia de una palabra clave en cada columna del DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    dataframe (pd.DataFrame): El DataFrame a analizar.\n",
    "    palabra_clave (str): La palabra clave que se busca en el DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Un DataFrame que muestra la cantidad y porcentaje de la palabra clave en cada columna.\n",
    "    \"\"\"\n",
    "    columnas_con_palabra_clave = dataframe.columns\n",
    "    resultados = []\n",
    "\n",
    "    for columna in columnas_con_palabra_clave:\n",
    "        cantidad_palabra_clave = dataframe[columna].eq(palabra_clave).sum()\n",
    "        porcentaje_palabra_clave = round((cantidad_palabra_clave / len(dataframe)) * 100, 2)\n",
    "        resultados.append({'Columna': columna, f'Cantidad de {palabra_clave}': cantidad_palabra_clave, f'% {palabra_clave} x Columna': porcentaje_palabra_clave})\n",
    "\n",
    "    resultados_df = pd.DataFrame(resultados)\n",
    "    resultados_con_palabra_clave = resultados_df[resultados_df[f'Cantidad de {palabra_clave}'] > 0]\n",
    "\n",
    "    return resultados_con_palabra_clave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def completar_coordenadas_con_comuna(dataframe):\n",
    "    \"\"\"\n",
    "    Completa los valores de longitud y latitud utilizando la información de la comuna en registros con valores NaN.\n",
    "\n",
    "    Parameters:\n",
    "    dataframe (pd.DataFrame): El DataFrame a transformar.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: El DataFrame con los valores de longitud y latitud actualizados.\n",
    "    \"\"\"\n",
    "    # Iterar sobre cada fila del DataFrame\n",
    "    for index, row in dataframe.iterrows():\n",
    "        # Verificar si la longitud y la latitud son NaN en la fila actual\n",
    "        if pd.isna(row['longitud']) and pd.isna(row['latitud']):\n",
    "            # Obtener la comuna de la fila actual\n",
    "            comuna = row['comuna']\n",
    "            \n",
    "            # Buscar otro registro con la misma comuna y valores de longitud y latitud no nulos\n",
    "            registro_similar = dataframe[(dataframe['comuna'] == comuna) & \n",
    "                                         (~dataframe['longitud'].isna()) & \n",
    "                                         (~dataframe['latitud'].isna())]\n",
    "            \n",
    "            # Verificar si se encontró un registro similar\n",
    "            if not registro_similar.empty:\n",
    "                # Tomar los valores de longitud y latitud del registro similar encontrado\n",
    "                longitud_similar = registro_similar.iloc[0]['longitud']\n",
    "                latitud_similar = registro_similar.iloc[0]['latitud']\n",
    "                \n",
    "                # Actualizar los valores de longitud y latitud en la fila actual\n",
    "                dataframe.at[index, 'longitud'] = longitud_similar\n",
    "                dataframe.at[index, 'latitud'] = latitud_similar\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analisis_categorico(dataframe, columna):\n",
    "    \"\"\"\n",
    "    Realiza un conteo de los valores únicos en la columna especificada del DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    dataframe (pd.DataFrame): El DataFrame que contiene los datos.\n",
    "    columna (str): El nombre de la columna que se utilizará para contar los valores.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Un DataFrame que muestra la frecuencia de cada valor único en la columna especificada.\n",
    "    \"\"\"\n",
    "    conteo_valores = dataframe[columna].value_counts().sort_index().reset_index()\n",
    "    conteo_valores.columns = [columna, 'Frecuencia']\n",
    "    return conteo_valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mysql_db(csv_file_path, db_name, table_name, host='localhost', user='tu_usuario', password='tu_contraseña'):\n",
    "    \"\"\"\n",
    "    Crea una base de datos MySQL y una tabla a partir de un archivo CSV.\n",
    "\n",
    "    Parameters:\n",
    "    - csv_file_path (str): Ruta del archivo CSV.\n",
    "    - db_name (str): Nombre de la base de datos a crear.\n",
    "    - table_name (str): Nombre de la tabla a crear.\n",
    "    - host (str, optional): Dirección del servidor MySQL. Por defecto, 'localhost'.\n",
    "    - user (str, optional): Usuario de MySQL. Por defecto, 'tu_usuario'.\n",
    "    - password (str, optional): Contraseña de MySQL. Por defecto, 'tu_contraseña'.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validaciones\n",
    "        if not csv_file_path.endswith('.csv'):\n",
    "            raise ValueError(\"El archivo debe tener extensión CSV.\")\n",
    "\n",
    "        # Cargar CSV en un DataFrame\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        \n",
    "        # Conectar a MySQL y crear la base de datos si no existe\n",
    "        connection = mysql.connector.connect(\n",
    "            host=host,\n",
    "            user=user,\n",
    "            password=password\n",
    "        )\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(f\"CREATE DATABASE IF NOT EXISTS {db_name}\")\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "\n",
    "        # Conectar a MySQL y crear la tabla si no existe\n",
    "        engine = create_engine(f'mysql+mysqlconnector://{user}:{password}@{host}:3306/{db_name}')\n",
    "        connection = engine.connect()\n",
    "        df.to_sql(table_name, connection, index=False, if_exists='replace')\n",
    "        connection.close()\n",
    "\n",
    "        print(\"Base de datos y tabla creadas exitosamente.\")\n",
    "    except pd.errors.EmptyDataError:\n",
    "        raise ValueError(\"El archivo CSV está vacío.\")\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Error al conectar a MySQL: {err}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error inesperado: {e}\")\n",
    "        raise\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
